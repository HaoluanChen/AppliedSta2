---
title: "Assignment2"
format: html
editor: visual
---

```{r}
library(readr)
library(tidyverse)
library(here)
library(rstan)
library(bayesplot) 
library(loo) 
library(tidybayes) 
```

# 1

## a)

From lecture 4: Assuming $\sigma$ is known, both likelihood and prior is normal, we have conjugate prior that our posterior also follows normal distribution:

$$p(\mu|y) \sim N(\mu_{posterior}, \sigma_{posterior}^2)$$ Where

$$\mu_{posterior} = N(\frac{\mu_0/\sigma_0^2+n*\bar{y}/\sigma^2}{1/\sigma_{\mu0}^2+n/\sigma^2}, \frac{1}{1/\sigma_{\mu0}^2+n/\sigma^2})$$

```{r}
mu0 = 100
sigma0 = 15
sigma = 15
mu = 113
n = 10

mu_posterior = (mu0/sigma0^2 + n*mu/sigma^2)/(1/sigma0^2+n/sigma^2)
mu_posterior
sigma_posterior = 1/(1/sigma0^2+n/sigma^2)
sigma_posterior
```

posterior mean is 111.82 and posterior variance is 20.45

```{r}
ci_lower <- qnorm(p = 0.025, mean = mu_posterior, sd = sqrt(sigma_posterior))
ci_upper <- qnorm(p = 0.975, mean = mu_posterior, sd = sqrt(sigma_posterior))
ci_lower
ci_upper
```

The 95% credible interval is \[102.95, 120.68\]

## b)

$$E[(\hat\mu-\mu^*)^2|\mu^*]= E[(\hat\mu-E(\hat\mu)+E(\hat\mu) -\mu^*)^2|\mu^*]$$

$$  = E[(\hat\mu-E(\hat\mu))^2 + 2(\hat\mu-E(\hat\mu))(E(\hat\mu)-\mu^*) + (E(\hat\mu) - \mu^*)^2|\mu^*]$$ $$ = E[(\hat\mu-E(\hat\mu))^2|\mu^*] + E[(E(\hat\mu)- \mu^*)|\mu^*]$$

## c)

Assuming $\mu^* =112$

For Bayesian: bias = 111.82 - 112 = 0.18 variance = 20.45 MSE = 20.45 + 0.18

For MLE:x\` Bias = 113 - 112 = 1 Variance = $\sigma^2/n = 22.5$ MSE = 1 + 22.5 = 23.5

MLE estimates has larger bias, variance, and MSE

## d)

```{r}
mle_mu <- 113 
mle_var <- 22.5
x <- seq(from = 90, to = 130, by = 0.1)
mle_data <- dnorm(x,mle_mu, sqrt(mle_var) )
bayes_data <- dnorm(x,mu_posterior, sqrt(sigma_posterior) )
data <- data.frame(x, mle_data, bayes_data)
data %>%
  pivot_longer(cols = c("mle_data","bayes_data"), names_to = "type", values_to = "density") %>%
ggplot(aes(x = x, y = density, fill = type)) +
geom_area(alpha = 0.8) +
labs(title = "Sampling densities by type")+
  geom_vline(xintercept = 112, color = "black")
```

```{r}
n = 50 
mle_mu <- 113 
mle_var <- 15^2/n
mu_posterior = (mu0/sigma0^2 + n*mu/sigma^2)/(1/sigma0^2+n/sigma^2)
sigma_posterior = 1/(1/sigma0^2+n/sigma^2)

```

```{r}
n = 100 
mle_mu <- 113 
mle_var <- 15^2/n
mu_posterior = (mu0/sigma0^2 + n*mu/sigma^2)/(1/sigma0^2+n/sigma^2)
sigma_posterior = 1/(1/sigma0^2+n/sigma^2)
```

```{r}
n <- seq(from = 10, to = 100, by = 1)
mle_mu <- 113 
mle_var <- 15^2/n
mu_posterior = (mu0/sigma0^2 + n*mu/sigma^2)/(1/sigma0^2+n/sigma^2)
sigma_posterior = 1/(1/sigma0^2+n/sigma^2)
d <- data.frame(n, mle_mu, mle_var,mu_posterior, sigma_posterior)
d %>% mutate(mle_bias = mle_mu - 112, bayes_bias = mu_posterior - 112, 
             mle_MSE = mle_bias+mle_var, bayes_MSE = bayes_bias+ sigma_posterior) %>% 
  ggplot(aes(x = n, y = bayes_MSE/mle_MSE)) + geom_point()
  
```

# 2

```{r}
sweden <- read_csv(here("data/sweden.csv"))
```

```{r}
sweden <- sweden %>% mutate(mortality = deaths/pop)
```

```{r}
sweden %>% filter(age>= 50 & age < 60) %>% 
  ggplot(aes(x = year, y = mortality, col = factor(age))) +   
           geom_point()

sweden %>% filter(age>= 60 & age < 70) %>% 
  ggplot(aes(x = year, y = mortality, col = factor(age))) +   
           geom_point()

sweden %>% filter(age>= 70 & age < 80) %>% 
  ggplot(aes(x = year, y = mortality, col = factor(age))) +   
           geom_point()

sweden %>% filter(age>= 80 & age < 90) %>% 
  ggplot(aes(x = year, y = mortality, col = factor(age))) +   
           geom_point()
sweden %>% filter(age >= 90) %>% 
  ggplot(aes(x = year, y = mortality, col = factor(age))) +   
           geom_point()
```

```{r}
sweden %>% filter(age == 100 & year == 2020)
sweden %>% filter(age == 50 & year == 2020)
sweden %>% filter(age == 75 & year == 2020)
```

```{r}
# find beta and alpha that can match mortality for each 2020, age 50 and age 100. These are beta and alpha should be at the tail of our prior distribution. Since age 50 has lowest mortality and 100 has highest mortality
x = 50
exp(x/90)/1000
x = 100
exp(x/16)/1000
```

```{r}
1/90
1/16

(1/90 + 1/16)/2
```

```{r}
# age = 50, beta = 1, alpha = 1000
exp(50)/1000
```

With beta \> 1, it would require really large alpha to scale it down, which means that our beta should be less than 1.

```{r}
# fix beta = 1/40 (in the middle between 1/90 and 1/16)
beta = 40
alpha <- seq(from = 1, to = 200, by = 2)
x = 50
y = exp(x/beta)/alpha
x50 <- data.frame(alpha,y)
x50 %>% ggplot(aes(alpha, exp(x/beta)/alpha))+ geom_point()
x = 100
y = exp(x/beta)/alpha
x100 <- data.frame(alpha,y)
x100 %>% ggplot(aes(alpha, exp(x/beta)/alpha))+ geom_point()
```

Firstly, I noticed that 0 \< $\beta$\< 1, lets put an uniform(0, 1) prior on the beta. Secondly, based on the second plot(age = 100) above, we see that when alpha is small we may see mortality rate above one, which is impossible. Therefore, the alpha is very unlikely to be below 10.

```{r}
x = 50
exp(x/90)/1000
```

Also, it is possible to have alpha value of 1000. Therefore, I will put a normal(700, 300) prior on the alpha.

```{r}
n = 2000
beta <- rnorm(n, 0.036, 0.03)
alpha <- rnorm(n, 700, 300)

age50 <- exp(50*beta)/alpha
age100 <- exp(100*beta)/alpha
q2b <- data.frame(age50,age100)

q2b %>% ggplot(aes(age50)) + geom_histogram(bins = 100) +  xlim(0, 0.8) +
  geom_vline(xintercept=0.001757909)

q2b %>% ggplot(aes(age100)) + geom_histogram(bins = 100) +  xlim(0, 0.8) + 
  geom_vline(xintercept=0.5960126)
```

```{r}
sum(age50 < 0.001757909)
```

## c)

Prior:

$$\alpha \sim N(700, 300)$$ $$\beta \sim N(0.036, 0.03)$$ Model:

$$D_x \sim logPoisson(\mu_xP_x)$$ 

where $\mu_x = \alpha e^{\beta x}$ 

$x$ refer to age 

$P_x$ refer to population


```{r}
sweden2020 <- sweden %>% filter(year == 2020)

mortality <- sweden2020$mortality
pop <- sweden2020$pop
age <- sweden2020$age

# named list to input for stan function
data <- list(N = length(mortality), 
             death = sweden2020$deaths,
             pop = pop,
             age = age)

fit3 <- stan(file = here("Assignments/Assignment2/2c_model.stan"),
            data = data, 
            iter = 5000, init = "random")
```

```{r}
fit3
```


# 3

```{r}
q3data <- read_table("http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat", col_names = c("id", "switch", "arsenic", "dist", "assoc", "educ"), skip = 1)
q3data <- q3data %>% mutate(switch_str = if_else(switch == 0, "Didn't Switch", "Switched"))
q3data
```


## a)

```{r}
q3data %>% ggplot(aes(x = arsenic, y = dist , col = as.factor(switch_str))) + 
  geom_point()+
  theme_bw() +
  geom_smooth(method = "lm")
```




```{r}
q3data %>% ggplot(aes(arsenic))+geom_histogram(bins=50)+facet_grid(.~switch_str)+
  theme_bw() 
```

```{r}
q3data %>% ggplot(aes(dist))+geom_histogram(bins=50)+facet_grid(.~switch_str)+
  theme_bw() 
```

## b)

```{r}
d <- q3data$dist - mean(q3data$dist)
a <- q3data$arsenic - mean(q3data$arsenic)
da <- d*a

# named list to input for stan function
data <- list(N = length(d), 
             d = d,
             a = a,
             da = da,
             y = q3data$switch)

fit1 <- stan(file = here("Assignments/Assignment2/3b1_model.stan"),
            data = data, 
            iter = 2000)
```




```{r}
d <- q3data$dist - mean(q3data$dist)
log_a <- log(q3data$arsenic) - mean(log(q3data$arsenic))
da <- d*a

# named list to input for stan function
data <- list(N = length(d), 
             d = d,
             a = log_a,
             da = da, 
            y = q3data$switch)

fit2 <- stan(file = here("Assignments/Assignment2/3b2_model.stan"),
            data = data, 
            iter = 2000)
```


```{r}
summary(fit1)$summary[c("beta[1]", "beta[2]", "beta[3]", "beta[4]"),]
```

```{r}
summary(fit2)$summary[c("beta[1]", "beta[2]", "beta[3]", "beta[4]"),]
```

## c)


```{r}
q3c <- as.data.frame(sapply(q3data, function(x) gsub("\"", "", x)))
ty_id <- q3c %>% filter(arsenic < 0.82) %>% mutate(id = as.numeric(noquote(id)))%>% pull(id)
ty <- sum(q3data$arsenic < 0.82 & q3data$switch == 1)/sum(q3data$arsenic < 0.82)
ty
```


```{r}
yrep1 <- extract(fit1)[["y_rep"]] %>% t() %>% as_tibble()
yrep2 <- extract(fit2)[["y_rep"]] %>% t() %>% as_tibble()
```

```{r}
yrep1
```

```{r}
ty_1 <- yrep1 %>% slice(ty_id) %>% summarise_all(mean)%>% t() %>% as.data.frame()
ty_2 <- yrep2 %>% slice(ty_id) %>% summarise_all(mean)%>% t() %>% as.data.frame()
```


```{r}
data.frame(ty_1, ty_2) %>% rename(model1 = V1, model2 = V1.1) %>% 
  pivot_longer(cols = everything(), names_to = "model", values_to = "ty") %>% 
  ggplot(aes(x = ty)) +
  geom_histogram(bins = 30, fill = "lightblue") +
  geom_vline(xintercept = ty, color = "red") +
  facet_wrap(.~model)
```
```{r}
log_lik_1 <- extract_log_lik(fit1)
log_lik_2 <- extract_log_lik(fit2)
loo1 <- loo(log_lik_1, save_psis = TRUE)
loo2 <- loo(log_lik_2, save_psis = TRUE)
loo_compare(loo1, loo2)
```

